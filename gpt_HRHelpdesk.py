from openai import OpenAI
from dotenv import load_dotenv
import os
import pandas as pd
import json
from pathlib import Path
import PyPDF2
import docx
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import pickle
from datetime import datetime

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')
client = OpenAI(api_key=api_key)

class DocumentRAG:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)
        self.documents = []
        self.embeddings = []
        self.doc_metadata = []
        
    def load_documents(self, folder_path):
        """í´ë”ì—ì„œ ë‹¤ì–‘í•œ í˜•íƒœì˜ ë¬¸ì„œë“¤ì„ ì½ì–´ì˜´"""
        folder = Path(folder_path)
        print(f"ğŸ“ ë¬¸ì„œ í´ë” ìŠ¤ìº” ì¤‘: {folder_path}")
        
        for file_path in folder.rglob('*'):
            if file_path.is_file():
                try:
                    content = self._read_file(file_path)
                    if content and len(content.strip()) > 50:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                        # ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
                        chunks = self._split_text(content, chunk_size=800, overlap=150)
                        for i, chunk in enumerate(chunks):
                            if len(chunk.strip()) > 30:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                                self.documents.append(chunk)
                                self.doc_metadata.append({
                                    'file_name': file_path.name,
                                    'file_path': str(file_path),
                                    'chunk_id': i,
                                    'total_chunks': len(chunks),
                                    'file_type': file_path.suffix.lower()
                                })
                        print(f"âœ… {file_path.name}: {len(chunks)}ê°œ ì²­í¬ ë¡œë“œ")
                except Exception as e:
                    print(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_path.name}: {e}")
        
        print(f"ğŸ“š ì´ {len(self.documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
        # ì²« ë²ˆì§¸ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°
        if self.documents:
            print(f"ğŸ“‹ ì²« ë²ˆì§¸ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°: {self.documents[0][:150]}...")
        
    def _read_file(self, file_path):
        """íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì½ê¸°"""
        extension = file_path.suffix.lower()
        
        try:
            if extension == '.txt':
                # ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
                for encoding in ['utf-8', 'cp949', 'euc-kr', 'latin-1']:
                    try:
                        with open(file_path, 'r', encoding=encoding) as f:
                            content = f.read()
                            return content
                    except UnicodeDecodeError:
                        continue
                return None
                
            elif extension == '.pdf':
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    text = ''
                    for page in reader.pages:
                        text += page.extract_text() + '\n'
                    return text
                    
            elif extension == '.docx':
                doc = docx.Document(file_path)
                return '\n'.join([paragraph.text for paragraph in doc.paragraphs])
                
            elif extension == '.csv':
                df = pd.read_csv(file_path)
                return df.to_string()
                
            elif extension == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return json.dumps(data, ensure_ascii=False, indent=2)
        
        except Exception as e:
            print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path.name}: {e}")
            return None
        
        return None
    
    def _split_text(self, text, chunk_size=800, overlap=150):
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (HR ë¬¸ì„œì— ìµœì í™”)"""
        chunks = []
        
        # ì„¹ì…˜ë³„ë¡œ ë¨¼ì € ë¶„í•  (HR ë¬¸ì„œëŠ” ë³´í†µ ì„¹ì…˜ì´ ìˆìŒ)
        sections = text.split('\n\n')
        current_chunk = ""
        
        for section in sections:
            if len(current_chunk) + len(section) < chunk_size:
                current_chunk += section + '\n\n'
            else:
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = section + '\n\n'
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        # ë„ˆë¬´ ê¸´ ì²­í¬ëŠ” ì¶”ê°€ ë¶„í• 
        final_chunks = []
        for chunk in chunks:
            if len(chunk) <= chunk_size:
                final_chunks.append(chunk)
            else:
                # ê¸´ ì²­í¬ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
                sentences = chunk.split('. ')
                temp_chunk = ""
                for sentence in sentences:
                    if len(temp_chunk) + len(sentence) < chunk_size:
                        temp_chunk += sentence + '. '
                    else:
                        if temp_chunk.strip():
                            final_chunks.append(temp_chunk.strip())
                        temp_chunk = sentence + '. '
                if temp_chunk.strip():
                    final_chunks.append(temp_chunk.strip())
        
        return final_chunks
    
    def create_embeddings(self):
        """ë¬¸ì„œë“¤ì˜ ì„ë² ë”© ìƒì„±"""
        if not self.documents:
            print("âŒ ì„ë² ë”©í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        print("ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘...")
        self.embeddings = self.embedding_model.encode(self.documents, show_progress_bar=True)
        print("âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
        
    def save_embeddings(self, file_path='hr_embeddings.pkl'):
        """ì„ë² ë”©ê³¼ ë¬¸ì„œ ë°ì´í„° ì €ì¥"""
        data = {
            'documents': self.documents,
            'embeddings': self.embeddings,
            'metadata': self.doc_metadata,
            'created_time': datetime.now().isoformat()
        }
        with open(file_path, 'wb') as f:
            pickle.dump(data, f)
        print(f"ğŸ’¾ ì„ë² ë”©ì´ {file_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    def load_embeddings(self, file_path='hr_embeddings.pkl'):
        """ì €ì¥ëœ ì„ë² ë”©ê³¼ ë¬¸ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
            self.documents = data['documents']
            self.embeddings = data['embeddings']
            self.doc_metadata = data['metadata']
            created_time = data.get('created_time', 'ì•Œ ìˆ˜ ì—†ìŒ')
            print(f"ğŸ“‚ ì„ë² ë”© ë¡œë“œ ì™„ë£Œ: {file_path}")
            print(f"ğŸ“… ìƒì„± ì‹œê°„: {created_time}")
            print(f"ğŸ“š ë¬¸ì„œ ìˆ˜: {len(self.documents)}ê°œ")
            return True
        except FileNotFoundError:
            print(f"ğŸ“ {file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            return False
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def search_similar_documents(self, query, top_k=4):
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        if not self.documents or len(self.embeddings) == 0:
            return []
            
        query_embedding = self.embedding_model.encode([query])
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤ ì°¾ê¸°
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.2:  # HR ê´€ë ¨ì„± ì„ê³„ê°’
                results.append({
                    'content': self.documents[idx],
                    'similarity': similarities[idx],
                    'metadata': self.doc_metadata[idx]
                })
        
        return results

class HRHelpdeskChatBot:
    def __init__(self, documents_folder=None):
        self.rag = DocumentRAG()
        self.conversation_history = []
        self.messages = [
            {"role": "system", "content": """ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  ì¹œê·¼í•œ HR(ì¸ì‚¬) í—¬í”„ë°ìŠ¤í¬ ë‹´ë‹¹ìì…ë‹ˆë‹¤. 
            ì§ì›ë“¤ì˜ ì¸ì‚¬ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì„ í•´ê²°í•˜ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ì£¼ëœ ì—­í• ì…ë‹ˆë‹¤.
            
            ì£¼ìš” ë‹´ë‹¹ ì—…ë¬´:
            â€¢ ê¸‰ì—¬, ìˆ˜ë‹¹, ì„¸ê¸ˆ ê´€ë ¨ ë¬¸ì˜
            â€¢ ì—°ì°¨, ë³‘ê°€, íœ´ê°€ ì •ì±… ì•ˆë‚´
            â€¢ ë³µë¦¬í›„ìƒ ì œë„ ì„¤ëª…
            â€¢ ì¸ì‚¬í‰ê°€, ìŠ¹ì§„ ê´€ë ¨ ì •ë³´
            â€¢ êµìœ¡/í›ˆë ¨ í”„ë¡œê·¸ë¨ ì•ˆë‚´
            â€¢ íšŒì‚¬ ì •ì±… ë° ê·œì • ì„¤ëª…
            â€¢ ì¡°ì§ ë³€ê²½, ì¸ì‚¬ì´ë™ ê´€ë ¨ ì•ˆë‚´
            â€¢ í‡´ì§, ì „ì§ ê´€ë ¨ ì ˆì°¨
            â€¢ ì§ì¥ ë‚´ ê³ ì¶© ìƒë‹´
            â€¢ ì‹ ì…ì‚¬ì› ì˜¨ë³´ë”© ì§€ì›
            
            ë‹µë³€ ê°€ì´ë“œë¼ì¸:
            1. ì œê³µëœ íšŒì‚¬ HR ë¬¸ì„œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬ ì •í™•í•œ ì •ë³´ ì œê³µ
            2. ë¯¼ê°í•œ ê°œì¸ì •ë³´ëŠ” ì§ì ‘ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ë‹´ë‹¹ ë¶€ì„œ ì—°ê²° ì•ˆë‚´
            3. ì •ì±…ì´ë‚˜ ê·œì •ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…
            4. ì ˆì°¨ê°€ ë³µì¡í•œ ê²½ìš° ë‹¨ê³„ë³„ë¡œ ì¹œì ˆí•˜ê²Œ ì•ˆë‚´
            5. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¼ë°˜ì ì¸ HR ì§€ì‹ìœ¼ë¡œ ë„ì›€ ì œê³µ
            6. í•­ìƒ ê³µì •í•˜ê³  ì¼ê´€ëœ ì •ë³´ ì œê³µ
            7. í•„ìš”ì‹œ ê´€ë ¨ ë¶€ì„œë‚˜ ë‹´ë‹¹ì ì—°ê²° ì•ˆë‚´
            8. ë¹„ë°€ìœ ì§€ì™€ ê°œì¸ì •ë³´ ë³´í˜¸ ì¤€ìˆ˜
            
            ë§íˆ¬: ì „ë¬¸ì ì´ë©´ì„œë„ ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ, ê³µê°ì ì¸ í†¤ìœ¼ë¡œ ì‘ë‹µí•˜ë˜ 
            íšŒì‚¬ì˜ í’ˆê²©ì„ ìœ ì§€í•˜ëŠ” ì •ì¤‘í•œ ì–¸ì–´ ì‚¬ìš©
            
            ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ë‹¤ë©´ ê·¸ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê³ , 
            ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ì¼ë°˜ì ì¸ HR ì§€ì‹ê³¼ ëª¨ë²” ì‚¬ë¡€ë¡œ ë„ì›€ì„ ë“œë¦¬ë˜ 
            ì •í™•í•œ íšŒì‚¬ ì •ì±…ì€ ë‹´ë‹¹ ë¶€ì„œì— í™•ì¸í•˜ë„ë¡ ì•ˆë‚´í•´ì£¼ì„¸ìš”."""}
        ]
        
        if documents_folder:
            self._setup_documents(documents_folder)
    
    def _setup_documents(self, folder_path):
        """ë¬¸ì„œ ì„¤ì • ë° ì„ë² ë”© ìƒì„±/ë¡œë“œ"""
        print(f"ğŸ“ HR ë¬¸ì„œ í´ë”: {folder_path}")
        print(f"ğŸ“‚ í´ë” ì¡´ì¬ ì—¬ë¶€: {os.path.exists(folder_path)}")
        
        # ê¸°ì¡´ ì„ë² ë”©ì´ ìˆëŠ”ì§€ í™•ì¸
        if not self.rag.load_embeddings():
            print("ğŸ”„ ìƒˆë¡œìš´ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤...")
            self.rag.load_documents(folder_path)
            if self.rag.documents:
                self.rag.create_embeddings()
                self.rag.save_embeddings()
            else:
                print("âš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ HR í—¬í”„ë°ìŠ¤í¬ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    def get_context_from_documents(self, user_input, top_k=4):
        """ì‚¬ìš©ì ì…ë ¥ê³¼ ê´€ë ¨ëœ HR ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        if not self.rag.documents:
            return ""
        
        similar_docs = self.rag.search_similar_documents(user_input, top_k=top_k)
        
        if not similar_docs:
            return ""
        
        context = "=== ê´€ë ¨ HR ì •ì±… ë° ë¬¸ì„œ ===\n\n"
        for i, doc in enumerate(similar_docs):
            context += f"[ì°¸ê³ ë¬¸ì„œ {i+1}] {doc['metadata']['file_name']}:\n"
            context += f"{doc['content']}\n"
            context += f"(ê´€ë ¨ë„: {doc['similarity']:.0%})\n\n"
        
        context += "=== ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš” ===\n"
        return context
    
    def save_conversation(self, user_input, ai_response):
        """ëŒ€í™” ë‚´ì—­ ì €ì¥"""
        conversation = {
            'timestamp': datetime.now().isoformat(),
            'user_input': user_input,
            'ai_response': ai_response
        }
        self.conversation_history.append(conversation)
        
        # íŒŒì¼ë¡œ ì €ì¥ (ì„ íƒì‚¬í•­)
        try:
            with open('hr_conversation_log.json', 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
        except:
            pass  # ì €ì¥ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
    
    def get_quick_help(self):
        """ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ ê°€ì´ë“œ"""
        return """
ğŸ” HR í—¬í”„ë°ìŠ¤í¬ - ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

ğŸ’° ê¸‰ì—¬/ìˆ˜ë‹¹ ê´€ë ¨:
   â€¢ "ê¸‰ì—¬ëª…ì„¸ì„œëŠ” ì–´ë””ì„œ í™•ì¸í•˜ë‚˜ìš”?"
   â€¢ "ì•¼ê·¼ìˆ˜ë‹¹ ì‹ ì²­ ë°©ë²•ì´ ê¶ê¸ˆí•´ìš”"
   â€¢ "ì„¸ê¸ˆê³µì œ í•­ëª©ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"

ğŸ–ï¸ íœ´ê°€/ì—°ì°¨ ê´€ë ¨:
   â€¢ "ì—°ì°¨ ì‹ ì²­ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
   â€¢ "ë³‘ê°€ ì‚¬ìš© ì¡°ê±´ì´ ê¶ê¸ˆí•´ìš”"
   â€¢ "ì¶œì‚°íœ´ê°€ ê¸°ê°„ì€ ì–¼ë§ˆë‚˜ ë˜ë‚˜ìš”?"

ğŸ ë³µë¦¬í›„ìƒ:
   â€¢ "ê±´ê°•ê²€ì§„ ì¼ì •ì„ ì•Œë ¤ì£¼ì„¸ìš”"
   â€¢ "ìë…€ í•™ìê¸ˆ ì§€ì› ì œë„ê°€ ìˆë‚˜ìš”?"
   â€¢ "ì§ì› í• ì¸ í˜œíƒì— ëŒ€í•´ ê¶ê¸ˆí•´ìš”"

ğŸ“ˆ ì¸ì‚¬í‰ê°€/ìŠ¹ì§„:
   â€¢ "ì¸ì‚¬í‰ê°€ ê¸°ì¤€ì´ ê¶ê¸ˆí•´ìš”"
   â€¢ "ìŠ¹ì§„ ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?"
   â€¢ "êµìœ¡ í”„ë¡œê·¸ë¨ ì‹ ì²­ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”"

ì›í•˜ì‹œëŠ” ì£¼ì œë¡œ ì–¸ì œë“  ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ™‚
        """
    
    def chat(self):
        """HR í—¬í”„ë°ìŠ¤í¬ ì±—ë´‡ ì‹¤í–‰"""
        print("ğŸ‘¥ HR í—¬í”„ë°ìŠ¤í¬ ì±—ë´‡ì…ë‹ˆë‹¤!")
        print("ì¸ì‚¬ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”. ğŸ˜Š")
        print("=" * 60)
        
        # ë„ì›€ë§ í‘œì‹œ
        print(self.get_quick_help())
        print("ğŸ’¬ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ë ¤ë©´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("ë„ì›€ë§ì„ ë‹¤ì‹œ ë³´ë ¤ë©´ 'help' ë˜ëŠ” 'ë„ì›€ë§'ì„ ì…ë ¥í•˜ì„¸ìš”.")
        print("=" * 60)
        
        while True:
            user_input = input("\nğŸ‘¤ ì§ì›: ").strip()
            
            if user_input.lower() in ["exit", "ì¢…ë£Œ", "quit"]:
                print("\nğŸ‘¥ HR í—¬í”„ë°ìŠ¤í¬: ë¬¸ì˜í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤! ì¶”ê°€ ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì—°ë½ì£¼ì„¸ìš”. ğŸ˜Š")
                break
            
            if user_input.lower() in ["help", "ë„ì›€ë§", "ë„ì›€"]:
                print(self.get_quick_help())
                continue
            
            if not user_input:
                print("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ë¬¸ì„œì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            context = self.get_context_from_documents(user_input)
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë©”ì‹œì§€ì— ì¶”ê°€
            if context:
                enhanced_input = f"{context}\n\nì§ì› ì§ˆë¬¸: {user_input}"
            else:
                enhanced_input = user_input
            
            # ëŒ€í™” ë‚´ì—­ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            self.messages.append({"role": "user", "content": enhanced_input})
            
            try:
                # OpenAI API í˜¸ì¶œ
                response = client.chat.completions.create(
                    model="gpt-4o",
                    temperature=0.3,  # HRì€ ì¼ê´€ë˜ê³  ì •í™•í•œ ë‹µë³€ì´ ì¤‘ìš”
                    messages=self.messages,
                    max_tokens=1200
                )
                
                # AI ì‘ë‹µ ì €ì¥ ë° ì¶œë ¥
                ai_message = response.choices[0].message.content
                self.messages.append({"role": "assistant", "content": ai_message})
                print(f"\nğŸ‘¥ HR í—¬í”„ë°ìŠ¤í¬: {ai_message}")
                
                # ì»¨í…ìŠ¤íŠ¸ê°€ ì‚¬ìš©ë˜ì—ˆë‹¤ë©´ ì¶œì²˜ í‘œì‹œ
                if context:
                    print("\nğŸ“‹ [ì°¸ê³  ë¬¸ì„œ]")
                    similar_docs = self.rag.search_similar_documents(user_input, top_k=3)
                    for doc in similar_docs:
                        if doc['similarity'] > 0.2:
                            print(f"   ğŸ“„ {doc['metadata']['file_name']} (ê´€ë ¨ë„: {doc['similarity']:.0%})")
                
                # ëŒ€í™” ì €ì¥
                self.save_conversation(user_input, ai_message)
                
                # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ (ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡)
                if len(self.messages) > 20:
                    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ ìµœê·¼ ëŒ€í™”ë§Œ ìœ ì§€
                    self.messages = [self.messages[0]] + self.messages[-15:]
                
            except Exception as e:
                print(f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                print("ğŸ”§ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì‹œê±°ë‚˜, ë‹´ë‹¹ ë¶€ì„œë¡œ ì§ì ‘ ì—°ë½í•´ì£¼ì„¸ìš”.")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ğŸ”¥ ì—¬ê¸°ì„œ HR ë¬¸ì„œ í´ë” ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”! ğŸ”¥
    hr_documents_folder = "C:/Users/SunnyPark/AI-Data/Clarios-HR"  # HR ë¬¸ì„œ í´ë” ê²½ë¡œ
    
    # ë‹¤ë¥¸ ê²½ë¡œ ì˜ˆì‹œë“¤:
    # hr_documents_folder = "./hr_documents"           # í˜„ì¬ í´ë” ì•ˆì˜ hr_documents í´ë”
    # hr_documents_folder = "D:/Company/HR_Policies"   # ë‹¤ë¥¸ ë“œë¼ì´ë¸Œì˜ HR ì •ì±… í´ë”
    # hr_documents_folder = None                       # ë¬¸ì„œ ì—†ì´ ì¼ë°˜ HR í—¬í”„ë°ìŠ¤í¬ë¡œ ì‹¤í–‰
    
    # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±í•˜ë¼ê³  ì•ˆë‚´
    if hr_documents_folder and not os.path.exists(hr_documents_folder):
        print(f"ğŸ“ HR ë¬¸ì„œ í´ë” '{hr_documents_folder}'ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("\në‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("1. í•´ë‹¹ í´ë”ë¥¼ ë§Œë“¤ê³  HR ë¬¸ì„œë“¤(.txt, .pdf, .docx, .csv, .json)ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        print("2. ë˜ëŠ” ì½”ë“œì—ì„œ hr_documents_folder ë³€ìˆ˜ë¥¼ ì˜¬ë°”ë¥¸ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”.")
        print("3. ë˜ëŠ” ë¬¸ì„œ ì—†ì´ ì‹¤í–‰í•˜ë ¤ë©´ hr_documents_folder = Noneìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        
        create_folder = input("\ní´ë”ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
        if create_folder == 'y':
            os.makedirs(hr_documents_folder)
            print(f"âœ… '{hr_documents_folder}' í´ë”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("HR ê´€ë ¨ ë¬¸ì„œë“¤ì„ ë„£ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
            exit()
        else:
            hr_documents_folder = None
    
    print("ğŸ¢ HR í—¬í”„ë°ìŠ¤í¬ ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("=" * 60)
    
    # HR í—¬í”„ë°ìŠ¤í¬ ì±—ë´‡ ì‹¤í–‰
    chatbot = HRHelpdeskChatBot(hr_documents_folder)
    chatbot.chat()
from openai import OpenAI
from dotenv import load_dotenv
import os
import pandas as pd
import json
from pathlib import Path
import PyPDF2
import docx
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import pickle

load_dotenv()
api_key = os.getenv('OPENAI_API_KEY')
client = OpenAI(api_key=api_key)

class DocumentRAG:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.embedding_model = SentenceTransformer(model_name)
        self.documents = []
        self.embeddings = []
        self.doc_metadata = []
        
    def load_documents(self, folder_path):
        """í´ë”ì—ì„œ ë‹¤ì–‘í•œ í˜•íƒœì˜ ë¬¸ì„œë“¤ì„ ì½ì–´ì˜´"""
        folder = Path(folder_path)
        
        for file_path in folder.rglob('*'):
            if file_path.is_file():
                try:
                    content = self._read_file(file_path)
                    if content:
                        # ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
                        chunks = self._split_text(content, chunk_size=1000, overlap=200)
                        for i, chunk in enumerate(chunks):
                            self.documents.append(chunk)
                            self.doc_metadata.append({
                                'file_name': file_path.name,
                                'file_path': str(file_path),
                                'chunk_id': i,
                                'total_chunks': len(chunks)
                            })
                except Exception as e:
                    print(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {file_path}: {e}")
        
        print(f"ì´ {len(self.documents)}ê°œì˜ ë¬¸ì„œ ì²­í¬ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
    def _read_file(self, file_path):
        """íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì½ê¸°"""
        extension = file_path.suffix.lower()
        
        if extension == '.txt':
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
                
        elif extension == '.pdf':
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text = ''
                for page in reader.pages:
                    text += page.extract_text()
                return text
                
        elif extension == '.docx':
            doc = docx.Document(file_path)
            return '\n'.join([paragraph.text for paragraph in doc.paragraphs])
            
        elif extension == '.csv':
            df = pd.read_csv(file_path)
            return df.to_string()
            
        elif extension == '.json':
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return json.dumps(data, ensure_ascii=False, indent=2)
        
        return None
    
    def _split_text(self, text, chunk_size=1000, overlap=200):
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸°
            if end < len(text):
                last_period = chunk.rfind('.')
                last_newline = chunk.rfind('\n')
                cut_point = max(last_period, last_newline)
                
                if cut_point > start + chunk_size // 2:
                    chunk = text[start:cut_point + 1]
                    end = cut_point + 1
            
            chunks.append(chunk.strip())
            start = end - overlap
            
        return chunks
    
    def create_embeddings(self):
        """ë¬¸ì„œë“¤ì˜ ì„ë² ë”© ìƒì„±"""
        print("ì„ë² ë”© ìƒì„± ì¤‘...")
        self.embeddings = self.embedding_model.encode(self.documents)
        print("ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
        
    def save_embeddings(self, file_path='embeddings.pkl'):
        """ì„ë² ë”©ê³¼ ë¬¸ì„œ ë°ì´í„° ì €ì¥"""
        data = {
            'documents': self.documents,
            'embeddings': self.embeddings,
            'metadata': self.doc_metadata
        }
        with open(file_path, 'wb') as f:
            pickle.dump(data, f)
        print(f"ì„ë² ë”©ì´ {file_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    def load_embeddings(self, file_path='embeddings.pkl'):
        """ì €ì¥ëœ ì„ë² ë”©ê³¼ ë¬¸ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(file_path, 'rb') as f:
                data = pickle.load(f)
            self.documents = data['documents']
            self.embeddings = data['embeddings']
            self.doc_metadata = data['metadata']
            print(f"ì„ë² ë”©ì´ {file_path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
        except FileNotFoundError:
            print(f"{file_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
    
    def search_similar_documents(self, query, top_k=3):
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        query_embedding = self.embedding_model.encode([query])
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        # ìƒìœ„ kê°œ ì¸ë±ìŠ¤ ì°¾ê¸°
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for idx in top_indices:
            results.append({
                'content': self.documents[idx],
                'similarity': similarities[idx],
                'metadata': self.doc_metadata[idx]
            })
        
        return results

class ITSupportChatBot:
    def __init__(self, documents_folder=None):
        self.rag = DocumentRAG()
        self.messages = [
            {"role": "system", "content": """ë„ˆëŠ” ì „ë¬¸ì ì´ê³  ì¹œê·¼í•œ IT ì§€ì›ë‹´ë‹¹ìì•¼. 
            ì‚¬ìš©ìì˜ IT ê´€ë ¨ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ê¸°ìˆ ì  ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ê²ƒì´ ì£¼ëœ ì—­í• ì´ì•¼.
            
            ë‹µë³€í•  ë•Œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì¤˜:
            1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬ ì •í™•í•œ ì •ë³´ ì œê³µ
            2. ê¸°ìˆ ì  ìš©ì–´ëŠ” ì‰½ê²Œ ì„¤ëª…í•˜ë˜ ì „ë¬¸ì„± ìœ ì§€
            3. ë‹¨ê³„ë³„ë¡œ êµ¬ì²´ì ì¸ í•´ê²° ë°©ë²• ì œì‹œ
            4. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¼ë°˜ì ì¸ IT ì§€ì‹ìœ¼ë¡œ ë„ì›€
            5. í•­ìƒ ì˜ˆì˜ë°”ë¥´ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì‘ë‹µ
            6. í•„ìš”ì‹œ ì¶”ê°€ ì •ë³´ë‚˜ í™•ì¸ ì‚¬í•­ ìš”ì²­
            
            ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ë‹¤ë©´ ê·¸ê²ƒì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê³ , 
            ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ì¼ë°˜ì ì¸ IT ì§€ì‹ìœ¼ë¡œ ì „ë¬¸ì ì´ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜."""}
        ]
        
        if documents_folder:
            self._setup_documents(documents_folder)
    
    def _setup_documents(self, folder_path):
        """ë¬¸ì„œ ì„¤ì • ë° ì„ë² ë”© ìƒì„±/ë¡œë“œ"""
        # ê¸°ì¡´ ì„ë² ë”©ì´ ìˆëŠ”ì§€ í™•ì¸
        if not self.rag.load_embeddings():
            print("ìƒˆë¡œìš´ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤...")
            self.rag.load_documents(folder_path)
            if self.rag.documents:
                self.rag.create_embeddings()
                self.rag.save_embeddings()
            else:
                print("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ì±—ë´‡ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    def get_context_from_documents(self, user_input, top_k=3):
        """ì‚¬ìš©ì ì…ë ¥ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
        if not self.rag.documents:
            return ""
        
        similar_docs = self.rag.search_similar_documents(user_input, top_k=top_k)
        
        context = "ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:\n"
        for i, doc in enumerate(similar_docs):
            if doc['similarity'] > 0.3:  # ìœ ì‚¬ë„ ì„ê³„ê°’
                context += f"\n[ë¬¸ì„œ {i+1}] {doc['metadata']['file_name']}:\n"
                context += f"{doc['content'][:500]}...\n"
        
        return context if len(context) > 20 else ""
    
    def chat(self):
        """IT ì§€ì› ì±—ë´‡ ì‹¤í–‰"""
        print("ğŸ–¥ï¸  IT ì§€ì› ì±—ë´‡ì…ë‹ˆë‹¤! ê¸°ìˆ ì  ë¬¸ì œë‚˜ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”.")
        print("(ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)")
        print("=" * 60)
        
        while True:
            user_input = input("\nğŸ‘¤ ì‚¬ìš©ì: ")
            if user_input.lower() == "exit":
                print("ğŸ–¥ï¸ IT ì§€ì›: ë¬¸ì œ í•´ê²°ì— ë„ì›€ì´ ë˜ì—ˆê¸°ë¥¼ ë°”ëë‹ˆë‹¤. ì–¸ì œë“  ë‹¤ì‹œ ë¬¸ì˜í•´ì£¼ì„¸ìš”! ğŸ˜Š")
                break
            
            # ë¬¸ì„œì—ì„œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            context = self.get_context_from_documents(user_input)
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë©”ì‹œì§€ì— ì¶”ê°€
            if context:
                enhanced_input = f"{context}\n\nì‚¬ìš©ì ì§ˆë¬¸: {user_input}"
            else:
                enhanced_input = user_input
            
            # ëŒ€í™” ë‚´ì—­ì— ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            self.messages.append({"role": "user", "content": enhanced_input})
            
            try:
                # OpenAI API í˜¸ì¶œ
                response = client.chat.completions.create(
                    model="gpt-4o",
                    temperature=0.7,  # IT ì§€ì›ì€ ì¡°ê¸ˆ ë” ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ ë‚®ì¶¤
                    messages=self.messages,
                    max_tokens=1000
                )
                
                # AI ì‘ë‹µ ì €ì¥ ë° ì¶œë ¥
                ai_message = response.choices[0].message.content
                self.messages.append({"role": "assistant", "content": ai_message})
                print(f"\nğŸ–¥ï¸ IT ì§€ì›: {ai_message}")
                
                # ì»¨í…ìŠ¤íŠ¸ê°€ ì‚¬ìš©ë˜ì—ˆë‹¤ë©´ ì¶œì²˜ í‘œì‹œ
                if context:
                    print("\nğŸ“‹ [ì°¸ê³  ë¬¸ì„œ]")
                    similar_docs = self.rag.search_similar_documents(user_input, top_k=3)
                    for doc in similar_docs:
                        if doc['similarity'] > 0.3:
                            print(f"   ğŸ“„ {doc['metadata']['file_name']} (ê´€ë ¨ë„: {doc['similarity']:.0%})")
                
            except Exception as e:
                print(f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                print("ğŸ”§ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ğŸ”¥ ì—¬ê¸°ì„œ ë¬¸ì„œ í´ë” ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ì„¸ìš”! ğŸ”¥
    # ì˜ˆì‹œë“¤:
    # documents_folder = "./documents"           # í˜„ì¬ í´ë” ì•ˆì˜ documents í´ë”
    documents_folder = "C:/Users/SunnyPark/AI-Data/Clarios-IT/txt_files_ocr"  # ìœˆë„ìš° ì ˆëŒ€ê²½ë¡œ
    # documents_folder = "/Users/ì‚¬ìš©ìëª…/Documents/ë‚´ë¬¸ì„œë“¤"    # ë§¥ ì ˆëŒ€ê²½ë¡œ
    # documents_folder = None                    # ë¬¸ì„œ ì—†ì´ ì¼ë°˜ ì±—ë´‡ìœ¼ë¡œ ì‹¤í–‰
    
   # documents_folder = "./documents"  # ğŸ‘ˆ ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•˜ì„¸ìš”!
    
    # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±í•˜ë¼ê³  ì•ˆë‚´
    if documents_folder and not os.path.exists(documents_folder):
        print(f"ë¬¸ì„œ í´ë” '{documents_folder}'ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("1. í•´ë‹¹ í´ë”ë¥¼ ë§Œë“¤ê³  ë¬¸ì„œë“¤(.txt, .pdf, .docx, .csv, .json)ì„ ë„£ì–´ì£¼ì„¸ìš”.")
        print("2. ë˜ëŠ” ì½”ë“œì—ì„œ documents_folder ë³€ìˆ˜ë¥¼ ì˜¬ë°”ë¥¸ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”.")
        print("3. ë˜ëŠ” ë¬¸ì„œ ì—†ì´ ì‹¤í–‰í•˜ë ¤ë©´ documents_folder = Noneìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
        
        create_folder = input("í´ë”ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower()
        if create_folder == 'y':
            os.makedirs(documents_folder)
            print(f"'{documents_folder}' í´ë”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì„œë“¤ì„ ë„£ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
            exit()
        else:
            documents_folder = None
    
    # ì±—ë´‡ ì‹¤í–‰
    chatbot = ITSupportChatBot(documents_folder)
    chatbot.chat()